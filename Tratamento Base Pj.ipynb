{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce65bf9-283d-40b3-932d-6397734b4e99",
   "metadata": {},
   "source": [
    "# ETL  - Tratamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7689c87-84c2-46dd-90a9-76afeead3cd7",
   "metadata": {},
   "source": [
    "### Importando as Lib Utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfb69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import DoubleType,IntegerType,StringType, DataType\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "import pyodbc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b30d3-cfbf-431a-b50e-411203fce30a",
   "metadata": {},
   "source": [
    "### Declarando Variaveis de Ambiente para Exporção em CSV no Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b28d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME']=r'C:\\Users\\leandro.silva\\PROJETOS\\BPC_By_Spark\\spark-3.2.4-bin-hadoop2.7'\n",
    "os.environ['JAVA_HOME']=r'C:\\Program Files\\Java\\jre1.8.0_172'\n",
    "os.environ['HADOOP_HOME']=r'C:\\Users\\leandro.silva\\PROJETOS\\BPC_By_Spark\\spark-3.2.4-bin-hadoop2.7\\hadoop'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466247f4-1c3d-4d23-a9eb-1cb0a27028e5",
   "metadata": {},
   "source": [
    "### Declarando Variaveis para Conexão com Banco de dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765b73e5-c4dd-4cf4-a05a-79b9e5b7be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'UNITNB015\\CONNECTION'\n",
    "database = 'EMPRESA_HOMOLOGACAO'\n",
    "user = 'sa'\n",
    "password = '85356325Ll@'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c440b5e8-ce86-4507-9c75-0af5c7d2d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conexao = pyodbc.connect\\\n",
    "            (f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={server};DATABASE={database};UID={user};PWD={password}'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5e2a5-3fff-4e55-9101-7b395e46c907",
   "metadata": {},
   "source": [
    "### Criando conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d297dc-a10c-4f1c-b4d5-086d28bc4af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conexao.cursor()\n",
    "cursor.fast_executemany = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587735c2-2898-4028-a3b7-e4ae73a72c67",
   "metadata": {},
   "source": [
    "### Criando Sessão do PySpark para Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f82686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://UNITNB015.UnitOffice.com.br:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Tratamento Base CNPJ</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ef62977650>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .master('local[*]')\\\n",
    "            .appName('Tratamento Base CNPJ')\\\n",
    "            .config('spark.ui.port','4051')\\\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe88283",
   "metadata": {},
   "source": [
    "## Importando Tabelas empresas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513daa3b-ba3b-41b2-8ca2-9d5df0e610c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = os.listdir('EMPRESAS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9928b1d7-9abf-4c66-afb8-299b75b31316",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = spark.read.csv(f'EMPRESAS/{arquivos[0]}', encoding='ISO-8859-1', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce288e2-03c6-43a0-8367-a1ce272538a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,arq in enumerate(arquivos):\n",
    "    file = spark.read.csv(f'EMPRESAS/{arq}', encoding='ISO-8859-1', sep=';')\n",
    "    empresas = empresas.union(file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b19b2-8b31-4e1a-97a0-4f6543f59468",
   "metadata": {},
   "source": [
    "### Deletando os Registros Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf2b1a7-cd9c-4c71-ad60-14333f8d29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = empresas.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1984189-2aba-4910-8c2a-91f074fc5304",
   "metadata": {},
   "source": [
    "### INSERINDO NOME DAS COLUNAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7855b3",
   "metadata": {},
   "source": [
    "| Campo |Descrição|\n",
    "|-------|-----------|\n",
    "|CNPJ BÁSICO | NÚMERO BASE DE INSCRIÇÃO NO CNPJ (OITO PRIMEIROS DÍGITOSDO CNPJ)\n",
    "RAZÃO SOCIAL |NOME EMPRESARIAL NOME EMPRESARIAL DA PESSOA JURÍDICA\n",
    "NATUREZA JURÍDICA | CÓDIGO DA NATUREZA JURÍDICA\n",
    "QUALIFICAÇÃO DO RESPONSÁVEL |QUALIFICAÇÃO DA PESSOA FÍSICA RESPONSÁVEL PELA EMPRESA\n",
    "CAPITAL SOCIAL DA EMPRESA | CAPITAL SOCIAL DA EMPRESA\n",
    "PORTE DA EMPRESA CÓDIGO DO PORTE DA EMPRESA:|00 – NÃO INFORMADO - 01 - MICRO EMPRESA - 03 - EMPRESA DE PEQUENO PORTE 05 - DEMAIS\n",
    "ENTE FEDERATIVO|RESPONSÁVEL O ENTE FEDERATIVO RESPONSÁVEL É PREENCHIDO PARA OS CASOS DE ÓRGÃOS E ENTIDADES DO GRUPO DE NATUREZA JURÍDICA 1XXX. PARA AS DEMAIS NATUREZAS, ESTE ATRIBUTO FICA EM BRANCO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb439130",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_colunas = ['CNPJ_BASICO','RAZAO_SOCIAL','NATUREZA_JURIDICA','QUALIFICACAO_DO_RESPONSAVEL',\n",
    "                    'CAPITAL_SOCIAL_DA_EMPRESA','PORTE','ENTE_FEDERATIVO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7e8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y  in enumerate(empresas_colunas):\n",
    "    empresas = empresas.withColumnRenamed(f'_c{x}',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03bc51",
   "metadata": {},
   "source": [
    "## Alterar o Porte para descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "818a5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "portes = {'00': 'NAO INFORMADO','01' : 'MICRO EMPRESA', '03':'EMPRESA DE PEQUENO PORTE', '05':'DEMAIS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e260650",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = empresas.withColumn('PORTE',f.when(f.isnull(empresas['PORTE']),portes['00'])\\\n",
    "                    .when(empresas['PORTE']=='01', portes['01'])\\\n",
    "                    .when(empresas['PORTE']=='03', portes['03'])\\\n",
    "                    .when(empresas['PORTE']=='05',portes['05'])\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e063ad-8e13-4d3f-adf5-12c9b493bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = empresas.withColumn\\\n",
    "            ('CAPITAL_SOCIAL_DA_EMPRESA',\n",
    "                     f.regexp_replace('CAPITAL_SOCIAL_DA_EMPRESA',',','.')\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68aab01b-a464-4b14-b6f2-45759c7f7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = empresas.withColumn\\\n",
    "        ('CAPITAL_SOCIAL_DA_EMPRESA',\n",
    "                     empresas['CAPITAL_SOCIAL_DA_EMPRESA'].cast(DoubleType())\n",
    "\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec9281",
   "metadata": {},
   "source": [
    "## Analisando Estabelecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35e595",
   "metadata": {},
   "source": [
    "|Campo | Descricao|\n",
    "|-------|----------|\n",
    "|CNPJ BASICO |NÚMERO BASE DE INSCRIÇÃO NO CNPJ (OITO PRIMEIROS DÍGITOS DO CNPJ)|\n",
    "|CNPJ | ORDEM NÚMERO DO ESTABELECIMENTO DE INSCRIÇÃO NO CNPJ (DO NONO ATÉ O DÉCIMO SEGUNDO DÍGITO DO CNPJ)|\n",
    "|CNPJ DV | DÍGITO VERIFICADOR DO NÚMERO DE INSCRIÇÃO NO CNPJ (DOIS ÚLTIMOS DÍGITOS DO CNPJ).\n",
    "|IDENTIFICADOR MATRIZ/FILIAL |CÓDIGO DO IDENTIFICADOR MATRIZ/FILIAL: 1 – MATRIZ 2 – FILIAL|\n",
    "|NOME FANTASIA |CORRESPONDE AO NOME FANTASIA\n",
    "|SITUAÇÃO CADASTRAL| CÓDIGO DA SITUAÇÃO CADASTRAL: 01 – NULA 2 – ATIVA 3 – SUSPENSA 4 – INAPTA 08 – BAIXADA|\n",
    "|DATA SITUAÇÃO CADASTRAL |DATA DO EVENTO DA SITUAÇÃO CADASTRAL|\n",
    "|MOTIVO SITUAÇÃO CADASTRAL | CÓDIGO DO MOTIVO DA SITUAÇÃO CADASTRAL|\n",
    "|NOME DA CIDADE NO EXTERIOR |NOME DA CIDADE NO EXTERIOR|\n",
    "|PAIS | CÓDIGO DO PAIS |\n",
    "|DATA DE INÍCIO | ATIVIDADE DATA DE INÍCIO DA ATIVIDADE|\n",
    "|CNAE FISCAL | PRINCIPAL CÓDIGO DA ATIVIDADE ECONÔMICA PRINCIPAL DO ESTABELECIMENTO|\n",
    "|CNAE FISCAL SECUNDÁRIA | CÓDIGO DA(S) ATIVIDADE(S) ECONÔMICA(S) SECUNDÁRIA(S) DO ESTABELECIMENTO|\n",
    "|TIPO DE LOGRADOURO | DESCRIÇÃO DO TIPO DE LOGRADOURO|\n",
    "|LOGRADOURO| NOME DO LOGRADOURO ONDE SE LOCALIZA O ESTABELECIMENTO.|\n",
    "|NÚMERO| NÚMERO ONDE SE LOCALIZA O ESTABELECIMENTO. QUANDO NÃO HOUVER PREENCHIMENTO DO NÚMERO HAVERÁ ‘S/N’.\n",
    "|COMPLEMENTO| COMPLEMENTO PARA O ENDEREÇO DE LOCALIZAÇÃO DO ESTABELECIMENTO|\n",
    "|BAIRRO |BAIRRO ONDE SE LOCALIZA O ESTABELECIMENTO.|\n",
    "|CEP | CÓDIGO DE ENDEREÇAMENTO POSTAL REFERENTE AO LOGRADOURO NO QUAL O ESTABELECIMENTO ESTA LOCALIZADO|\n",
    "|UF| SIGLA DA UNIDADE DA FEDERAÇÃO EM QUE SE ENCONTRA O ESTABELECIMENTO|\n",
    "|MUNICÍPIO | CÓDIGO DO MUNICÍPIO DE JURISDIÇÃO ONDE SE ENCONTRA O ESTABELECIMENTO|\n",
    "|DDD 1| CONTÉM O DDD 1|\n",
    "|TELEFONE 1 |CONTÉM O NÚMERO DO TELEFONE 1|\n",
    "|DDD 2 |CONTÉM O DDD 2|\n",
    "|TELEFONE 2 | CONTÉM O NÚMERO DO TELEFONE 2|\n",
    "|DDD DO FAX | CONTÉM O DDD DO FAX|\n",
    "|FAX| CONTÉM O NÚMERO DO FAX|\n",
    "|CORREIO ELETRÔNICO |CONTÉM O E-MAIL DO CONTRIBUINTE|\n",
    "|SITUAÇÃO ESPECIAL |SITUAÇÃO ESPECIAL DA EMPRESA|\n",
    "|DATA DA SITUAÇÃO ESPECIAL| DATA EM QUE A EMPRESA ENTROU EM SITUAÇÃO ESPECIAL|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81697c15",
   "metadata": {},
   "source": [
    "## Importar tabela Estabelecimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d180074-f766-42f9-ab21-a99fab211562",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = os.listdir('ESTABELECIMENTO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e02ed7b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estabelecimento = spark.read.csv(f'ESTABELECIMENTO/{arquivos[0]}', sep=';', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c69df624-f1a5-417b-a7e1-304373c53348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c,arq in enumerate(arquivos):\n",
    "    file = spark.read.csv(f'ESTABELECIMENTO/{arq}', encoding='ISO-8859-1', sep=';')\n",
    "    estabelecimento = estabelecimento.union(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97e8292f-5b0e-47a5-8709-08b1d11b6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "estabelecimento = estabelecimento.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "780769b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "estabele_coluna = ['CNPJ_BASICO','CNPJ','CNPJ_DV','IDENTIFICADOR_MATRIZ/FILIAL','NOME_FANTASIA','SITUACAO_CADASTRAL',\n",
    "'DATA_SITUACAO_CADASTRAL','MOTIVO_SITUACAO_CADASTRAL','NOME_DA_CIDADE_NO_EXTERIOR','PAIS','DT_ABERTURA',\n",
    "    'CNAE_FISCAL','CNAE_FISCAL_SECUNDARIA','TIPO_DE_LOGRADOURO','LOGRADOURO','NUMERO','COMPLEMENTO','BAIRRO',\n",
    "'CEP','UF','MUNICIPIO','DDD_1','TELEFONE_1','DDD_2','TELEFONE_2','DDD DO_FAX','FAX','CORREIO_ELETRONICO',\n",
    "'SITUACAO_ESPECIAL','DATA_DA_SITUACAO_ESPECIAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a085c",
   "metadata": {},
   "source": [
    "## Renomeando colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5462dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in enumerate(estabele_coluna):\n",
    "    estabelecimento = estabelecimento\\\n",
    "            .withColumnRenamed(f'_c{x}',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c79c40",
   "metadata": {},
   "source": [
    "## Criado coluna unificada do CNPJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "066e1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estabelecimento = estabelecimento.select('CNPJ_BASICO',f.concat('CNPJ_BASICO','CNPJ','CNPJ_DV').alias('CNPJ'),\n",
    "                      'IDENTIFICADOR_MATRIZ/FILIAL',\n",
    "                     'NOME_FANTASIA',\n",
    "                     'SITUACAO_CADASTRAL',\n",
    "                     'DATA_SITUACAO_CADASTRAL',\n",
    "                     'MOTIVO_SITUACAO_CADASTRAL',\n",
    "                     'NOME_DA_CIDADE_NO_EXTERIOR',\n",
    "                     'PAIS',\n",
    "                     'DT_ABERTURA',\n",
    "                     'CNAE_FISCAL',\n",
    "                     'CNAE_FISCAL_SECUNDARIA',\n",
    "                     'TIPO_DE_LOGRADOURO',\n",
    "                     'LOGRADOURO',\n",
    "                     'NUMERO',\n",
    "                     'COMPLEMENTO',\n",
    "                     'BAIRRO',\n",
    "                     'CEP',\n",
    "                     'UF',\n",
    "                     'MUNICIPIO',\n",
    "                     'DDD_1',\n",
    "                     'TELEFONE_1',\n",
    "                     'DDD_2',\n",
    "                     'TELEFONE_2',\n",
    "                     'DDD DO_FAX',\n",
    "                     'FAX',\n",
    "                     'CORREIO_ELETRONICO',\n",
    "                     'SITUACAO_ESPECIAL',\n",
    "                     'DATA_DA_SITUACAO_ESPECIAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca861eb",
   "metadata": {},
   "source": [
    "## Juntar base estabecimento com empresas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4eaaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "coluna_empresa_nova = ['CNPJ_BASICO',\n",
    "'CNPJ',\n",
    "'RAZAO_SOCIAL',\n",
    "'NOME_FANTASIA',\n",
    "'IDENTIFICADOR_MATRIZ/FILIAL',\n",
    "'SITUACAO_CADASTRAL',\n",
    "'NATUREZA_JURIDICA',\n",
    "'QUALIFICACAO_DO_RESPONSAVEL',\n",
    "'CAPITAL_SOCIAL_DA_EMPRESA',\n",
    "'PORTE',\n",
    "'DATA_SITUACAO_CADASTRAL',\n",
    "'MOTIVO_SITUACAO_CADASTRAL',\n",
    "'NOME_DA_CIDADE_NO_EXTERIOR',\n",
    "'PAIS',\n",
    "'DT_ABERTURA',\n",
    "'CNAE_FISCAL',\n",
    "'CNAE_FISCAL_SECUNDARIA',\n",
    "'SITUACAO_ESPECIAL',\n",
    "'DATA_DA_SITUACAO_ESPECIAL',\n",
    "'ENTE_FEDERATIVO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c37abcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas.join(estabelecimento,on='CNPJ_BASICO', how='inner')\\\n",
    "                .select([x for x in coluna_empresa_nova])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbbdd7ee-574a-4154-99ea-1f48f83b5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.withColumn(\n",
    "    'DT_ABERTURA',empresas_nova['DT_ABERTURA'].cast(StringType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a8b70ae-b9ab-425b-997c-a8b61903c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.withColumn(\n",
    "    'DT_ABERTURA', f.to_date(empresas_nova['DT_ABERTURA'],'yyyyMMdd')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91c11fe8-9e31-4e1e-93ec-d6af7bf4f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.withColumn(\n",
    "    'DATA_DA_SITUACAO_ESPECIAL',empresas_nova['DATA_DA_SITUACAO_ESPECIAL'].cast(StringType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff44606e-494f-4f83-98c8-62ba82f7e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.withColumn(\n",
    "    'DATA_DA_SITUACAO_ESPECIAL', f.to_date(empresas_nova['DATA_DA_SITUACAO_ESPECIAL'],'yyyyMMdd')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "248a879f-40c1-45f4-a1ba-b1ef64deb511",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.withColumn(\n",
    "    'DATA_SITUACAO_CADASTRAL',empresas_nova['DATA_SITUACAO_CADASTRAL'].cast(StringType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc69c3e5-3ecc-4858-aca9-02100c40dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.withColumn(\n",
    "    'DATA_SITUACAO_CADASTRAL', f.to_date(empresas_nova['DATA_SITUACAO_CADASTRAL'],'yyyyMMdd')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27fecbc1-60f9-4d77-9dd8-1ecb23a8eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.withColumn('IDENTIFICADOR_MATRIZ/FILIAL',f.when(empresas_nova['IDENTIFICADOR_MATRIZ/FILIAL']=='1','M')\\\n",
    "                            .when(empresas_nova['IDENTIFICADOR_MATRIZ/FILIAL']=='2','F')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b97c935-3709-4de4-ab9e-df37ce922c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.drop('CNPJ_BASICO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "807ff122-6fda-48a3-a83b-44698a748bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova =  empresas_nova.withColumn('RAZAO_SOCIAL',f.regexp_replace('RAZAO_SOCIAL',';',' '))\\\n",
    "                    .withColumn('NOME_FANTASIA',f.regexp_replace('NOME_FANTASIA',';',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "529a3d3d-82bb-4f4e-9e51-90c4676e6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova =  empresas_nova.withColumn('RAZAO_SOCIAL',f.regexp_replace('RAZAO_SOCIAL',',',' '))\\\n",
    "                    .withColumn('NOME_FANTASIA',f.regexp_replace('NOME_FANTASIA',',',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a63d7e6e-3225-43f3-9dd0-964e970fdfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_cnae_secundario = empresas_nova.select('CNPJ','CNAE_FISCAL_SECUNDARIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b68faeee-23b5-4b2c-b1b7-bbb7ec24c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova = empresas_nova.withColumn(\n",
    "    'CNAE_FISCAL_SECUNDARIA',f.substring('CNAE_FISCAL_SECUNDARIA',1,7)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536a08c-4e6d-4e54-a595-970c96945b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_nova.coalesce(1).write.csv(mode='overwrite',path='ETL/EMPRESAS_ETL_EMPRESA_ARQ_UNICO/', sep=',',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b1acd46-9252-4d92-96db-84f9afb585e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CNPJ: string, RAZAO_SOCIAL: string, NOME_FANTASIA: string, IDENTIFICADOR_MATRIZ/FILIAL: string, SITUACAO_CADASTRAL: string, NATUREZA_JURIDICA: string, QUALIFICACAO_DO_RESPONSAVEL: string, CAPITAL_SOCIAL_DA_EMPRESA: double, PORTE: string, DATA_SITUACAO_CADASTRAL: date, MOTIVO_SITUACAO_CADASTRAL: string, NOME_DA_CIDADE_NO_EXTERIOR: string, PAIS: string, DT_ABERTURA: date, CNAE_FISCAL: string, CNAE_FISCAL_SECUNDARIA: string, SITUACAO_ESPECIAL: string, DATA_DA_SITUACAO_ESPECIAL: date, ENTE_FEDERATIVO: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empresas_nova.unpersist(blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0a76f-ced7-4ed5-8d79-581b1bda697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo = [arquivo for arquivo in os.listdir('ETL/EMPRESAS_ETL_EMPRESA_ARQ_UNICO') if '.csv' in arquivo and 'crc' not in arquivo][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdc07e-5865-4359-8cfc-2cd7d2c3716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979bda7-dc99-466f-9aa0-c4c8c87ac8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_generator = pd.read_csv(f'ETL/EMPRESAS_ETL_EMPRESA_ARQ_UNICO/{arquivo}',sep=',', chunksize=12000, on_bad_lines='skip' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8e8b5-94eb-48e4-b882-7a101aae6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''INSERT INTO EMPRESA_HOMOLOGACAO..EMPRESAS(CNPJ\n",
    "                                                    ,RAZAO_SOCIAL\n",
    "                                                    ,NOME_FANTASIA\n",
    "                                                    ,[IDENTIFICADOR_MATRIZ FILIAL]\n",
    "                                                    ,SITUACAO_CADASTRAL\n",
    "                                                    ,NATUREZA_JURIDICA\n",
    "                                                    ,QUALIFICACAO_DO_RESPONSAVEL\n",
    "                                                    ,CAPITAL_SOCIAL_DA_EMPRESA\n",
    "                                                    ,PORTE\n",
    "                                                    ,DATA_SITUACAO_CADASTRAL\n",
    "                                                    ,MOTIVO_SITUACAO_CADASTRAL\n",
    "                                                    ,NOME_DA_CIDADE_NO_EXTERIOR\n",
    "                                                    ,PAIS\n",
    "                                                    ,DT_ABERTURA\n",
    "                                                    ,CNAE_FISCAL\n",
    "                                                    ,CNAE_FISCAL_SECUNDARIA\n",
    "                                                    ,SITUACAO_ESPECIAL\n",
    "                                                    ,DATA_DA_SITUACAO_ESPECIAL\n",
    "                                                    ,ENTE_FEDERATIVO\n",
    ") \n",
    "VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec1da9-c2da-4ae5-a102-88e9bb08087f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for chunk in chunk_generator:\n",
    "    df = pd.DataFrame(chunk)\n",
    "    df = df.fillna('')\n",
    "    df = df.astype(str)\n",
    "    df['CNPJ'] = df['CNPJ'].str.zfill(14)\n",
    "    values  = [tuple(row) for _, row in df.iterrows()]\n",
    "    cursor.executemany (query,values)\n",
    "    cursor.commit()\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2717981-3acd-4eaa-abd7-4af5c2e3ef93",
   "metadata": {},
   "source": [
    "### Criar tabela Empresa Cnae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ecddddb-9e11-440f-9b8d-aa8b57c45e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_cnae_top10000 = empresas_cnae_secundario.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68dd47ed-9ca8-46ea-b858-f100d1965de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o443.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 22.0 failed 1 times, most recent failure: Lost task 4.0 in stage 22.0 (TID 26) (UNITNB015.UnitOffice.com.br executor driver): java.io.FileNotFoundException: C:\\Users\\leandro.silva\\AppData\\Local\\Temp\\blockmgr-2a8bdaa9-55cb-45a9-b13b-1c1b4a775d14\\0e\\temp_local_d875de89-d83b-4125-b484-619c81faf669 (O sistema não pode encontrar o caminho especificado)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:291)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.<init>(UnsafeSorterSpillWriter.java:81)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:227)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:113)\r\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\r\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.agg_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\leandro.silva\\AppData\\Local\\Temp\\blockmgr-2a8bdaa9-55cb-45a9-b13b-1c1b4a775d14\\0e\\temp_local_d875de89-d83b-4125-b484-619c81faf669 (O sistema não pode encontrar o caminho especificado)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:291)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.<init>(UnsafeSorterSpillWriter.java:81)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:227)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:113)\r\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\r\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.agg_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[43mempresas_cnae_top10000\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCNPJ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCNAE_FISCAL_SECUNDARIA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PROJETOS\\ETL_PJ\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\PROJETOS\\ETL_PJ\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\PROJETOS\\ETL_PJ\\venv\\Lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\PROJETOS\\ETL_PJ\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o443.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 22.0 failed 1 times, most recent failure: Lost task 4.0 in stage 22.0 (TID 26) (UNITNB015.UnitOffice.com.br executor driver): java.io.FileNotFoundException: C:\\Users\\leandro.silva\\AppData\\Local\\Temp\\blockmgr-2a8bdaa9-55cb-45a9-b13b-1c1b4a775d14\\0e\\temp_local_d875de89-d83b-4125-b484-619c81faf669 (O sistema não pode encontrar o caminho especificado)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:291)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.<init>(UnsafeSorterSpillWriter.java:81)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:227)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:113)\r\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\r\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.agg_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\leandro.silva\\AppData\\Local\\Temp\\blockmgr-2a8bdaa9-55cb-45a9-b13b-1c1b4a775d14\\0e\\temp_local_d875de89-d83b-4125-b484-619c81faf669 (O sistema não pode encontrar o caminho especificado)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:291)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.<init>(UnsafeSorterSpillWriter.java:81)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:227)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.createWithExistingInMemorySorter(UnsafeExternalSorter.java:113)\r\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:158)\r\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.agg_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "empresas_cnae_top10000.select('CNPJ','CNAE_FISCAL_SECUNDARIA')\\\n",
    "                    .limit(10)\\\n",
    "                    .show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a23d424a-76f9-40c1-9d52-c131528949f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o478.showString.\n: java.nio.file.NoSuchFileException: C:\\Users\\leandro.silva\\AppData\\Local\\Temp\\blockmgr-2a8bdaa9-55cb-45a9-b13b-1c1b4a775d14\\32\r\n\tat sun.nio.fs.WindowsException.translateToIOException(Unknown Source)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(Unknown Source)\r\n\tat java.nio.file.Files.createDirectory(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:96)\r\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:137)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:879)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1964)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:370)\r\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1385)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeBlocks$1(TorrentBroadcast.scala:150)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeBlocks$1$adapted(TorrentBroadcast.scala:144)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:144)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:74)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1525)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:122)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:177)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:426)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:417)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:504)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)\r\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:237)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\r\n\tat org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:698)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:698)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:146)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:170)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:170)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:172)\r\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:258)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:256)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:256)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:228)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:370)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:343)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mempresas_cnae_secundario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PROJETOS\\ETL_PJ\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\PROJETOS\\ETL_PJ\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\PROJETOS\\ETL_PJ\\venv\\Lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\PROJETOS\\ETL_PJ\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o478.showString.\n: java.nio.file.NoSuchFileException: C:\\Users\\leandro.silva\\AppData\\Local\\Temp\\blockmgr-2a8bdaa9-55cb-45a9-b13b-1c1b4a775d14\\32\r\n\tat sun.nio.fs.WindowsException.translateToIOException(Unknown Source)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(Unknown Source)\r\n\tat java.nio.file.Files.createDirectory(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:96)\r\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:137)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:879)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1964)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:370)\r\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1385)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeBlocks$1(TorrentBroadcast.scala:150)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeBlocks$1$adapted(TorrentBroadcast.scala:144)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:144)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:74)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1525)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:122)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:177)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:426)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:417)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:504)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)\r\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:237)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\r\n\tat org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:698)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:698)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:526)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:497)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:146)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:750)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:170)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:170)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:172)\r\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:258)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:256)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:256)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:228)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:370)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:343)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "empresas_cnae_secundario.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58a001-de79-417c-9363-943c4549ba02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2d709e8-e5a8-4192-ba64-92293e2875ed",
   "metadata": {},
   "source": [
    "## Criar tabela endereço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02542a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coluna_endereco_nova = ['CNPJ',\n",
    "    'TIPO_DE_LOGRADOURO',\n",
    "    'LOGRADOURO',\n",
    "    'NUMERO',\n",
    "    'COMPLEMENTO',\n",
    "    'BAIRRO',\n",
    "    'CEP',\n",
    "    'UF',\n",
    "    'MUNICIPIO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae79d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "coluna_telefones_nova = [\n",
    "    'CNPJ',\n",
    "    'DDD_1',\n",
    "    'TELEFONE_1',\n",
    "    'DDD_2',\n",
    "    'TELEFONE_2',\n",
    "    'DDD DO_FAX',\n",
    "    'FAX'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ab15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coluna_email_nova = [\n",
    "    'CNPJ',\n",
    "    'CORREIO_ELETRÔNICO'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26998a",
   "metadata": {},
   "source": [
    "## Tratamento e criacao da base endereço Receita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c454ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "endereco_nova = estabelecimento.select([x for x in coluna_endereco_nova])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d931c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "endereco_nova.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7417c30",
   "metadata": {},
   "source": [
    "## Deletar qualquer registros que não tenha CEP de 8 Digito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59480d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "endereco_nova = endereco_nova.select('*')\\\n",
    "            .where(f.length('CEP')==8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6e5a7-44b5-4aa0-9e2f-2c123ede4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acentos = {\n",
    "    'á': 'a', 'à': 'a', 'â': 'a', 'ã': 'a', 'ä': 'a',\n",
    "    'é': 'e', 'è': 'e', 'ê': 'e', 'ë': 'e',\n",
    "    'í': 'i', 'ì': 'i', 'î': 'i', 'ï': 'i',\n",
    "    'ó': 'o', 'ò': 'o', 'ô': 'o', 'õ': 'o', 'ö': 'o',\n",
    "    'ú': 'u', 'ù': 'u', 'û': 'u', 'ü': 'u',\n",
    "    'ç': 'c', 'ñ': 'n'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316bf42e-3ae6-4800-bdea-21ba7cb01b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776b389-8f24-458e-ab9b-21393b6c576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acentos_pattern = \"|\".join(map(re.escape, acentos.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ddb23-8b06-4e11-a03f-7c8869cacf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_acento_udf = f.udf(remover_acento,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acaa06-82b3-4042-baef-779885cfdb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "endereco_nova.withColumn('LOGRADOURO', f.regexp_replace(f.col('LOGRADOURO'),acentos_pattern,lambda x: acentos[x.group(0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e359e7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endereco_nova.select('TIPO_DE_LOGRADOURO')\\\n",
    "            .groupBy('TIPO_DE_LOGRADOURO')\\\n",
    "            .agg(f.count('*').alias('qtd'))\\\n",
    "            .orderBy('qtd',ascending=False)\\\n",
    "            .show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "endereco_nova.select('CEP')\\\n",
    "            .drop_duplicates()\\\n",
    "            .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb95ec",
   "metadata": {},
   "source": [
    "## Validar todos os CEPs e depois de ter todos os CEPs validados atualizar endereço e deletar  os CEPS Invalidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceps_validar = endereco_nova.select('CEP')\\\n",
    "                .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abfffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceps_validar.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95732274",
   "metadata": {},
   "source": [
    "## Tratamento e criacao da base Telefone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova = estabelecimento.select([x for x in coluna_telefones_nova])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1654c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova.select('*')\\\n",
    "            .limit(10)\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524b798",
   "metadata": {},
   "source": [
    "## Deletar numeros menor que 8 digitos Tel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42211909",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova = telefone_nova.select('*')\\\n",
    "                        .where(f.length('TELEFONE_1') >= 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1524b",
   "metadata": {},
   "source": [
    "## Deletar numeros maior que 9 Digitos Tel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a55ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova = telefone_nova.select('*')\\\n",
    "                        .where(f.length('TELEFONE_1') <=9 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebf8ea",
   "metadata": {},
   "source": [
    "## Deletar numeros menor que 8 digitos Tel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova.select('*')\\\n",
    "        .where(f.length('TELEFONE_2') < 8)\\\n",
    "        .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915f158-a92b-401e-bd97-2337119b4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install phonenumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68052a9b-13e5-42da-a1cb-80459c9721cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonenumbers as phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ce2db-4802-4bc8-9820-8ae08fefe301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valida_telefone(numero_testar):\n",
    "    numero = phone.parse(f'+55{numero_testar}')\n",
    "    valido = phone.is_valid_number(numero)\n",
    "    possivel = phone.is_possible_number(numero)\n",
    "    return valido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ef5cd-7299-40ee-9baf-f6ae64276433",
   "metadata": {},
   "outputs": [],
   "source": [
    "valida_telefone('79529488')==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3efcdf-41c0-4302-a64e-3c207a201131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3fa79-54da-4676-a4da-ec545638d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova.withColumn(\n",
    "    'TELEFONE_10',f.when(valida_telefone(telefone_nova['']) col=telefone_nova['TELEFONE_1'])\n",
    ")\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c789ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova.select('*')\\\n",
    "        .where(f.length('TELEFONE_2') < 8)\\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d309dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova = telefone_nova\\\n",
    "                    .withColumn('TELEFONE_2',f.when(f.length('TELEFONE_2')< 8,None))\\\n",
    "                    .withColumn('DDD_2',f.when(f.length('TELEFONE_2')< 8,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova = telefone_nova\\\n",
    "            .withColumn('TELEFONE_2',f.when(f.length('TELEFONE_2') > 9,None))\\\n",
    "            .withColumn('DDD_2',f.when(f.length('TELEFONE_2')> 9,None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7354ff",
   "metadata": {},
   "source": [
    "## Atribuir o numero 9 em celulares com 8 Digitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd348e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova\\\n",
    "        .select('*')\\\n",
    "        .where((f.substring('TELEFONE_1',1,1) >= 9) & (f.length('TELEFONE_1')==9))\\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43702195",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova\\\n",
    "        .withColumn\\\n",
    "            ('TELEFONE_1',f.when((f.substring('TELEFONE_1',1,1) >= 6) & (f.length('TELEFONE_1')==8)\n",
    "                                 ,f.concat(f.lit('9'),'TELEFONE_1')))\\\n",
    "            .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091403b-d2f6-4fba-b3ca-a23129558fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova = telefone_nova\\\n",
    "        .withColumn\\\n",
    "            ('TELEFONE_1',f.when((f.substring('TELEFONE_1',1,1) >= 6) & (f.length('TELEFONE_1')==8)\n",
    "                                 ,f.concat(f.lit('9'),'TELEFONE_1')))\\\n",
    "        .withColumn\\\n",
    "               (\n",
    "                   'TELEFONE_2',f.when((f.substring('TELEFONE_2',1,1) >= 6) & (f.length('TELEFONE_2')==8),\n",
    "                               f.concat(f.lit('9'),'TELEFONE_2'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dd5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova.select('*')\\\n",
    "        .where(f.length('TELEFONE_2') >= 8)\\\n",
    "        .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e765292",
   "metadata": {},
   "outputs": [],
   "source": [
    "telefone_nova.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e8d16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fb38ccf",
   "metadata": {},
   "source": [
    "## Tratamento base Email Receita"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7106040d",
   "metadata": {},
   "source": [
    "## Analistando Tabela de CNAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20270ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnaes = spark.read.csv('Cnaes/',encoding='ISO-8859-1',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53027c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnaes = cnaes.withColumnRenamed('_c0','Codigo_cnae')\\\n",
    "            .withColumnRenamed('_c1','Descricao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnaes.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnaes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b89aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_novos = estabelecimento\\\n",
    "                    .select([x for x in coluna_email_nova])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_novos\\\n",
    "        .select('*')\\\n",
    "        .limit(10)\\\n",
    "        .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd6942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
